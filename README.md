

RAG-Based Multi-Modal Generative AI Assistant

### ğŸŒŸ What It Is

Agent-Nesh is an **AI-powered personal assistant** that can understand and respond to **text, images, code, and voice**.
Unlike a simple chatbot, it uses **Retrieval-Augmented Generation (RAG)** and multiple specialized AI models to provide **context-aware, multimodal answers**.

Think of it like a **supercharged ChatGPT** that can also:
âœ… Read and analyze images
âœ… Help you debug/write code
âœ… Understand spoken voice input
âœ… Provide more factual and grounded responses using retrieval

---

# ğŸ¯ Key Features

1. **Text Assistance** ğŸ“

   * General Q&A, summaries, explanations
   * Works like ChatGPT but uses **RAG** to fetch external/contextual data

2. **Code Assistance** ğŸ’»

   * Debugging help
   * Code generation (Python, JS, etc.)
   * UML diagram â†’ code conversion (via `uml_to_code.py` tool)

3. **Image Analysis** ğŸ–¼ï¸

   * Upload an image â†’ model analyzes and describes it
   * Can detect objects, scenes, and context

4. **Voice Recognition** ğŸ¤

   * Converts spoken language to text using **OpenAI Whisper**
   * Lets you interact by speaking instead of typing

---

# ğŸ§  AI Models Used

* **Meta Llama 3** â†’ General text generation & reasoning
* **Microsoft Phi 3 Vision** â†’ Vision + text understanding (images & multimodal input)
* **IBM Granite** â†’ Advanced NLP tasks and reasoning
* **OpenAI Whisper** â†’ Speech-to-text for voice input

Each model is wrapped in Python scripts (`llama.py`, `phi_vision.py`, etc.) so the assistant can choose the right tool for the task.

---

# ğŸ›ï¸ Project Structure (Simplified)

```
Generative agent/
â”œâ”€â”€ models/                # Wrappers for AI models
â”œâ”€â”€ chains/                # Task-specific chains (text, code, vision)
â”œâ”€â”€ utils/                 # Utility functions (e.g., image processing)
â”œâ”€â”€ agent/                 # Core agent logic
â”‚   â”œâ”€â”€ tools/             # Tools like UML â†’ code generator
â”‚   â””â”€â”€ llm_agent.py       # The main orchestrator
â””â”€â”€ app.py                 # Streamlit app entry point
```

ğŸ”‘ **Flow**:
User input â†’ Chosen chain (language, code, vision) â†’ Correct model â†’ Response back via Streamlit UI.

---

# ğŸš€ How to Run It

1. **Clone repo**

   ```bash
   git clone https://github.com/ganeshnehru/RAG-Multi-Modal-Generative-AI-Agent.git
   cd RAG-Multi-Modal-Generative-AI-Agent
   ```

2. **Create virtual environment**

   ```bash
   python3 -m venv venv
   source venv/bin/activate     # Windows: venv\Scripts\activate
   ```

3. **Install dependencies**

   ```bash
   pip install -r requirements.txt
   ```

4. **Set up environment variables** in `.env` file

   ```ini
   NVIDIA_API_KEY=your_nvidia_key
   OPENAI_API_KEY=your_openai_key
   ```

5. **Run app**

   ```bash
   streamlit run app.py
   ```

6. **Open browser** at `http://localhost:8501` â†’ Interact with Agent-Nesh.

---

# ğŸ› ï¸ Usage Examples

* **Ask a question** â†’ *â€œSummarize this articleâ€* â†’ Llama 3 responds.
* **Coding help** â†’ *â€œWrite a Python function for binary searchâ€* â†’ Code assistant responds.
* **Upload an image** â†’ *â€œWhatâ€™s in this picture?â€* â†’ Phi Vision describes it.
* **Voice input** â†’ Say *â€œWhatâ€™s the weather in NYC today?â€* â†’ Whisper transcribes â†’ Agent answers.

---

# ğŸ”— Tech Stack

* **Backend AI** â†’ Llama 3, Phi Vision, Granite, Whisper
* **Framework** â†’ Streamlit (for UI)
* **RAG** â†’ Retrieval system for grounding answers
* **Tools** â†’ Custom (UML â†’ code, image processor)

---

# ğŸ¯ Why Itâ€™s Useful

* Combines **multiple AI models** into one assistant
* Handles **multimodal input** (text, voice, image, code)
* Easy to run locally (just Python + Streamlit)
* Extensible â†’ you can add more models/tools

---

ğŸ‘‰ In short: **Agent-Nesh is like ChatGPT on steroids â€” a multi-modal, RAG-powered assistant that can chat, code, analyze images, and understand speech.**


